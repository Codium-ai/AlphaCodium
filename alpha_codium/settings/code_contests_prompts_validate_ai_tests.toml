[code_contests_prompts_validate_ai_tests]
temperature = 0.2
system = """\
Your goal is to consider each AI-generated test, and make sure its output and its explanation are correct. Be critical - they could be wrong.
guidelines:
- Read carefully the problem description. Make sure the output and the explanations are consistent with them, and between themselves.
- Make sure you understand problem constraints, rules, and examples.
- The tests explanations must coherently and logically lead from the input to the output.
"""

User="""\
You are given a code contest problem and a self-reflection on the problem:


problem description:
======
{{ description|trim }}
======


self-reflection on the problem:
======
{{ self_reflection|trim }}
======


Here are additional tests for the problem, generated by an AI:

AI-generated tests:
============
{{ problem_ai_tests|trim }}
============


Your goal is to consider each AI-generated test, and make sure the output and the explanation are correct. Be critical - they could be wrong.

Guidelines:
- Read the problem description carefully. Make sure the output and the explanations are consistent with them, and between themselves.
- The test explanations must coherently and logically lead from the input to the output.

The output must be a YAML object equivalent to type $ProblemTests, according to the following Pydantic definitions:
=====
Class Test(BaseModel):
    input: str
    output: str
    explanation: str = Field(description="Short explanation of how the input leads to the output.")

class ProblemTests(BaseModel):
    tests: List[Test] = Field(min_items={{number_of_ai_tests}}, max_items={{number_of_ai_tests}})
=====


Example YAML output:
```yaml
tests:
- input: |
    ...
  output: |
    ...
  explanation: |
    ...
...
```

Each YAML output MUST be after a newline, indented, with block scalar indicator ('|').

Answer:
```yaml\
"""