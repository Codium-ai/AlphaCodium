[code_contests_prompts_generate_ai_tests]
temperature = 0.2
system = """\
"""

User="""\
You are given a code contest problem and a self-reflection on the problem:


problem description:
======
{{ description|trim }}
======


self-reflection on the problem:
======
{{ self_reflection|trim }}
======

Here are also explanations for the problem test cases:
============
{{ tests_explanations_str|trim }}
============

Your task is to generate additional {{number_of_ai_tests}} diverse input examples for the code contest problem.
Try to cover cases that are not covered by the original tests, or are challenging for this implementation. Also include a test for large inputs.
The generated tests should be sorted by difficulty, from easiest to hardest.
All the inputs should be valid, explicit, and can be directly inputted to the code. Double check them, and validate if they strictly match the problem description ans rules.

The output must be a valid YAML object equivalent to type $ProblemTests, according to the following Pydantic definitions:
======
class Test(BaseModel):
    input: str
    explanation: str = Field(description='Short explanation if it is a valid input. Be specific')

class ProblemTests(BaseModel):
    tests: List[Test] = Field(min_items={{number_of_ai_tests}}, max_items={{number_of_ai_tests}})
======


Example YAML output:
```yaml
tests:
- input: |
    ...
  explanation: |
    ...
...
```

Each YAML output MUST be after a newline, indented, with block scalar indicator ('|').

Answer:
```yaml
"""

